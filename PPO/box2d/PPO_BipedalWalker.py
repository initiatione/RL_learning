from matplotlib import pyplot as plt
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import seaborn as sns
import random
import os


# åŠ¨ä½œå±‚
class ActorNet(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256):
        super(ActorNet, self).__init__()

        def _layer_init(layer, std=np.sqrt(2), bias_const=0.0):
            nn.init.orthogonal_(layer.weight, std)
            nn.init.constant_(layer.bias, bias_const)
            return layer

        """ self.actor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Tanh(),
        ) """

        self.actor = nn.Sequential(
            _layer_init(nn.Linear(input_dim, hidden_dim)),
            nn.ReLU(),
            _layer_init(nn.Linear(hidden_dim, hidden_dim)),
            nn.ReLU(),
            _layer_init(
                nn.Linear(hidden_dim, output_dim), std=0.01
            ),  # è¾“å‡ºå±‚å¢ç›Šè®¾å°ï¼Œåˆå§‹æ›´ç¨³å®š
            nn.Tanh(),
        )
        # self.log_std = nn.Parameter(torch.zeros(1, output_dim))
        # log_std åˆå§‹è®¾ä¸º -0.5 (stdçº¦ä¸º0.6)ï¼Œç»™æœºå™¨äººè¶³å¤Ÿçš„åˆå§‹æ¢ç´¢ç©ºé—´
        self.log_std = nn.Parameter(torch.full((1, output_dim), -0.5))

    def forward(self, x):
        mu = self.actor(x)
        std = torch.exp(self.log_std)
        return mu, std


# è¯„ä»·å±‚
class CriticNet(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256):
        super(CriticNet, self).__init__()
        assert output_dim == 1  # Critic åªèƒ½è¾“å‡ºä¸€ä¸ªæ ‡é‡åˆ†æ•°å€¼ï¼Œè¡¨ç¤ºè¯¥çŠ¶æ€çš„å¥½å
        self.critic = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
        )

    def forward(self, x):
        return self.critic(x)


class ReplayBuffer:
    def __init__(self, capacity, state_dim, action_dim, is_discrete=True, device="cpu"):
        self.capacity = capacity
        self.device = device
        self.is_discrete = is_discrete
        self.ptr = 0
        self.size = 0

        # é¢„åˆ†é…ç©ºé—´
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        if is_discrete:
            self.actions = np.zeros(capacity, dtype=np.int64)  # ç¦»æ•£åŠ¨ä½œ
        else:
            # è¿ç»­åŠ¨ä½œå­˜çš„æ˜¯å‘é‡
            self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.bool_)

    def push(self, state, action, reward, next_state, done):
        self.states[self.ptr] = state
        self.actions[self.ptr] = action  # Numpy ä¼šè‡ªåŠ¨å¤„ç†æ ‡é‡æˆ–å‘é‡çš„èµ‹å€¼
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = done

        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)

    def sample(self, batch_size):
        ind = np.random.randint(0, self.size, size=batch_size)

        # è½¬æ¢åŠ¨ä½œ Tensor æ—¶æ ¹æ®ç±»å‹é€‰æ‹© LongTensor æˆ– FloatTensor
        action_tensor = (
            torch.LongTensor(self.actions[ind])
            if self.is_discrete
            else torch.FloatTensor(self.actions[ind])
        )
        return (
            torch.FloatTensor(self.states[ind]).to(self.device),
            action_tensor.to(self.device),
            torch.FloatTensor(self.rewards[ind]).to(self.device),
            torch.FloatTensor(self.next_states[ind]).to(self.device),
            torch.FloatTensor(self.dones[ind]).to(self.device),
        )

    def clear(self):
        self.ptr = 0
        self.size = 0


class PPOBuffer(ReplayBuffer):
    def __init__(self, capacity, state_dim, action_dim, is_discrete=True, device="cpu"):
        super().__init__(capacity, state_dim, action_dim, is_discrete, device)
        self.log_probs = np.zeros(capacity, dtype=np.float32)
        self.values = np.zeros(capacity, dtype=np.float32)

    def push(self, state, action, reward, next_state, done, log_prob, value):
        # å…ˆåˆ©ç”¨çˆ¶ç±»çš„å­˜å‚¨é€»è¾‘
        idx = self.ptr  # è®°å½•å½“å‰å­˜åˆ°äº†å“ª
        super().push(state, action, reward, next_state, done)

        # è¡¥å…… PPO ç‰¹æœ‰çš„æ•°æ®
        self.log_probs[idx] = log_prob
        self.values[idx] = value

    def get_all(self):
        """PPO ä¸“ç”¨ï¼šä¸€æ¬¡æ€§å–å‡ºæ‰€æœ‰æ•°æ®ç”¨äºè®¡ç®— Returns å’Œ GAE"""
        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯æ•´ä¸ª Buffer é‡Œçš„æ•°æ®ï¼Œä¸æ‰“ä¹±é¡ºåº
        act_tensor = (
            torch.LongTensor(self.actions[: self.size])
            if self.is_discrete
            else torch.FloatTensor(self.actions[: self.size])
        )

        data = {
            "states": torch.FloatTensor(self.states[: self.size]).to(self.device),
            "actions": act_tensor.to(self.device),
            "log_probs": torch.FloatTensor(self.log_probs[: self.size]).to(self.device),
            "rewards": self.rewards[: self.size],
            "dones": self.dones[: self.size],
            "values": self.values[: self.size],
        }
        return data


class Agent:
    def __init__(self, cfg):
        self.gamma = cfg.gamma
        self.gae_lambda = cfg.gae_lambda  # GAE çš„å¹³æ»‘å‚æ•° lambda
        self.device = torch.device(cfg.device)
        self.k_epochs = cfg.k_epochs  # PPOæ›´æ–°è½®æ¬¡
        self.eps_clip = cfg.eps_clip  # è£å‡èŒƒå›´
        self.entropy_coef = cfg.entropy_coef  # ç†µç³»æ•°
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = ActorNet(cfg.n_states, cfg.n_actors, cfg.n_hidden_dim).to(
            self.device
        )
        self.critic = CriticNet(cfg.n_states, 1, cfg.n_hidden_dim).to(self.device)
        # ç½‘ç»œä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(
            self.actor.parameters(), lr=cfg.actor_lr
        )
        self.critic_optimizer = torch.optim.Adam(
            self.critic.parameters(), lr=cfg.critic_lr
        )
        # ç»éªŒå›æ”¾æ± 
        self.memory = PPOBuffer(
            capacity=cfg.batch_size,
            state_dim=cfg.n_states,
            action_dim=cfg.n_actions,
            is_discrete=cfg.is_discrete,
            device=self.device,
        )

    @torch.no_grad()
    def sample_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        mu, std = self.actor(state)
        dist = torch.distributions.Normal(mu, std)
        action = dist.sample()
        # é™åˆ¶åŠ¨ä½œèŒƒå›´åœ¨ [-1, 1]ï¼Œé˜²æ­¢è¶Šç•Œå¯¼è‡´çš„æ•°å€¼é—®é¢˜
        action = torch.clamp(action, -1.0, 1.0)
        log_prob = dist.log_prob(action).sum(dim=-1)
        # Criticè¯„ä¼°çš„å½“å‰çŠ¶æ€çš„Vå€¼
        value = self.critic(state)
        return action.cpu().numpy().flatten(), log_prob.item(), value.item()

    @torch.no_grad()
    def predict_action(self, state):
        """ç¡®å®šæ€§åŠ¨ä½œé¢„æµ‹ï¼šç”¨äºæµ‹è¯•å’Œéƒ¨ç½²"""
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        mu, std = self.actor(state)
        # å–æ¦‚ç‡æœ€å¤§çš„åŠ¨ä½œï¼Œè€Œä¸æ˜¯éšæœºé‡‡æ ·
        # action = torch.argmax(probs, dim=1)
        return mu.cpu().numpy().flatten()

    def evaluate(self, state, action):
        mu, std = self.actor(state)
        dist = torch.distributions.Normal(mu, std)
        log_probs = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().sum(dim=-1)
        state_values = self.critic(state)
        return log_probs, state_values, entropy

    def update(self):
        samples = self.memory.get_all()

        old_states = samples["states"]
        old_actions = samples["actions"]
        old_log_probs = samples["log_probs"]
        old_values = samples["values"]
        rewards = samples["rewards"]
        dones = samples["dones"]
        """ # MCæ¢¯åº¦ç®—æ³•
        returns = []
        discounted_sum = 0
        for reward, done in zip(reversed(rewards), reversed(dones)):
            # ç»“æŸåˆ™æœªæ¥å¥–åŠ±ä¸º0
            if done:
                discounted_sum = 0
            discounted_sum = reward + self.gamma * discounted_sum
            returns.insert(0, discounted_sum)

        returns = torch.FloatTensor(returns).to(self.device)
        advantages = returns - torch.FloatTensor(old_values).to(self.device)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) """

        # GAE
        advantages = []
        gae = 0
        last_state = samples["states"][-1].unsqueeze(0)
        with torch.no_grad():
            last_value = self.critic(last_state).item()
        next_value = last_value if not dones[-1] else 0
        for i in reversed(range(len(rewards))):
            mask = 1.0 - dones[i]
            # TD error
            delta = rewards[i] + self.gamma * next_value * mask - old_values[i]
            # GAE delta + gamma * lambda * mask * gae
            gae = delta + self.gamma * self.gae_lambda * mask * gae
            advantages.insert(0, gae)
            next_value = old_values[i]
        # è½¬ä¸ºTensor
        advantages = torch.FloatTensor(advantages).to(self.device)
        # è®¡ç®—ç›®æ ‡å›æŠ¥ Returns = Advantages + Values
        # returns ä½œä¸ºCriticçš„å­¦ä¹ ç›®æ ‡
        returns = advantages + torch.FloatTensor(old_values).to(self.device)
        # å½’ä¸€åŒ– Advantage
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        for _ in range(self.k_epochs):
            # é‡æ–°è¯„ä¼°å½“å‰ç½‘ç»œåœ¨æ—§çŠ¶æ€ä¸‹çš„è¡¨ç°
            # æ³¨æ„ï¼šè¿™é‡Œçš„ log_probs æ˜¯æœ‰æ¢¯åº¦çš„
            curr_log_probs, state_values, dist_entropy = self.evaluate(
                old_states, old_actions
            )
            # è®¡ç®—æ–°æ—§ç­–ç•¥æ¦‚ç‡æ¯”ï¼šratio = exp(log_new - log_old)
            ratio = torch.exp(curr_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = (
                torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            )
            # Actor Loss: è´Ÿå·æ˜¯å› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–å¥–åŠ±
            actor_loss = -torch.min(surr1, surr2).mean()
            # Critic Loss: å‡æ–¹è¯¯å·®ï¼Œè®© Critic ä¼°å€¼æ›´å‡†
            critic_loss = nn.MSELoss()(state_values.squeeze(), returns)
            # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + ä»·å€¼æŸå¤± - ç†µæ”¶ç›Š (é¼“åŠ±æ¢ç´¢)
            loss = (
                actor_loss + 0.5 * critic_loss - self.entropy_coef * dist_entropy.mean()
            )

            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.actor_optimizer.step()
            self.critic_optimizer.step()
        self.memory.clear()


class Config:
    def __init__(self):
        self.env_name = "BipedalWalker-v3"
        self.algo_name = "PPO"
        self.seed = 42
        # self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = "cpu"
        self.train_eps = 5000  # æ€»è®­ç»ƒå›åˆæ•°
        self.max_steps = 1600  # æ¯å›åˆæœ€å¤§æ­¥æ•°
        self.batch_size = 2048  # ç§¯ç´¯å¤šå°‘æ­¥æ•°æ®è¿›è¡Œä¸€æ¬¡ PPO æ›´æ–°
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.actor_lr = 0.0003
        self.critic_lr = 0.0003
        self.k_epochs = 10  # æ¯æ¬¡æ›´æ–°æ—¶å‹æ¦¨æ•°æ®çš„æ¬¡æ•°
        self.eps_clip = 0.2
        # åˆå§‹åŒ–ç†µç³»æ•°
        self.entropy_coef = 0.03
        self.entropy_min = 0.001
        self.entropy_decay = 0.999 # æ¯æ¬¡æ›´æ–°åè¡°å‡å› å­
        # éšè—å±‚
        self.n_hidden_dim = 256
        self.is_discrete = False  # ç¦»æ•£åŠ¨ä½œç©ºé—´
        self.eval_freq = 50  # è¯„ä¼°é¢‘ç‡
        self.eval_episodes = 5


def train(cfg, env, agent):
    print(f"å¼€å§‹è®­ç»ƒç¯å¢ƒ: {cfg.env_name} åœ¨è®¾å¤‡: {cfg.device}")
    rewards_history = []
    eval_rewards = []  # è®°å½•è¯„ä¼°å¾—åˆ†
    running_steps = 0
    best_reward = -np.inf

    for episode in range(1, cfg.train_eps + 1):
        state, info = env.reset()
        episode_reward = 0
        for _ in range(cfg.max_steps):
            running_steps += 1
            action, log_probs, value = agent.sample_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            
            # trick1
            modified_reward = reward
            if reward <=-100:
                modified_reward = -1.0
            # trick2 done or dead åŒºåˆ†
            # trick1 env_done ä¸ºç¯å¢ƒé‡å¯ä¿¡å· done
            env_done = terminated or truncated
            # åªæœ‰æ‘”æ­»çš„æ—¶å€™æ‰ä¸»åŠ¨ç»“æŸ dead
            buffer_done = True if reward <= -100 else False
            
            # å­˜replay buffer
            agent.memory.push(state, action, modified_reward, next_state, buffer_done, log_probs, value)
            state = next_state
            episode_reward += reward
            # æ•°æ®ç§¯ç´¯åˆ°ä¸€å®šé‡ï¼Œè¿›è¡Œä¸€æ¬¡ PPO æ›´æ–°
            if running_steps % cfg.batch_size == 0:
                agent.update()
            if env_done:
                break
        agent.entropy_coef = max(agent.entropy_coef * cfg.entropy_decay, cfg.entropy_min)
        # è®°å½•æœ¬å›åˆå¥–åŠ±
        rewards_history.append(episode_reward)

        if episode % cfg.eval_freq == 0:
            avg_reward = evaluate_policy(agent, cfg)
            eval_rewards.append(avg_reward)

            print("-" * 27)
            print(f"| Episode:        {episode:7} |")
            print(f"| Total Steps:    {running_steps:7} |")
            print(f"| Train Reward:   {episode_reward:7.2f} |")
            print(f"| Eval Reward:    {avg_reward:7.2f} |")

            if avg_reward > best_reward:
                best_reward = avg_reward
                torch.save(agent.actor.state_dict(), f"best_model_hardcore_{cfg.env_name}.pth")
                print(f"| æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            print("-" * 27)
    return rewards_history


def evaluate_policy(agent, cfg):
    eval_env = gym.make(cfg.env_name)
    #eval_env = gym.make(cfg.env_name, hardcore=True)
    
    avg_reward = 0.0
    for _ in range(cfg.eval_episodes):
        state, info = eval_env.reset()
        done = False
        ep_reward = 0
        while not done:
            action = agent.predict_action(state)
            state, reward, terminated, truncated, _ = eval_env.step(action)
            done = terminated or truncated
            ep_reward += reward
        avg_reward += ep_reward

    eval_env.close()
    return avg_reward / cfg.eval_episodes


def test(cfg, agent):
    print("\n--- å¼€å§‹åŠ è½½æœ€ä½³æ¨¡å‹æ¼”ç¤º ---")
    model_path = f"best_model_{cfg.env_name}.pth"
    if not os.path.exists(model_path):
        print(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶{model_path}ï¼è¯·å…ˆè®­ç»ƒã€‚")
        return

    # 1. å¿…é¡»ä½¿ç”¨ render_mode="human" æ‰èƒ½çœ‹åˆ°ç”»é¢
    test_env = gym.make(cfg.env_name, render_mode="human")
    # test_env = gym.make(cfg.env_name, hardcore=True, render_mode="human")
    # 2. åŠ è½½æ¨¡å‹æƒé‡
    state_dict = torch.load(model_path, map_location=cfg.device)
    agent.actor.load_state_dict(state_dict)
    agent.actor.eval()  # åˆ‡æ¢åˆ°é¢„æµ‹æ¨¡å¼

    # 3. è¿è¡Œæ¼”ç¤º
    for i in range(3):  # æ¼”ç¤º3æ¬¡
        state, _ = test_env.reset()
        done = False
        total_reward = 0
        while not done:
            action = agent.predict_action(state)
            state, reward, terminated, truncated, _ = test_env.step(action)
            done = terminated or truncated
            total_reward += reward
        print(f"æ¼”ç¤ºå›åˆ {i+1}: å¥–åŠ± {total_reward:.2f}")

    test_env.close()


def set_seed(seed, env=None):
    """å…¨å±€ç§å­è®¾å®šï¼Œä¿è¯å®éªŒå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if env is not None:
        env.action_space.seed(seed)
        env.observation_space.seed(seed)


def env_agent_config(cfg, render_mode=None):
    """
    æ™ºèƒ½é…ç½®å‡½æ•°ï¼šè‡ªåŠ¨æ£€æµ‹ç»´åº¦ï¼Œé€‚é…è®¾å¤‡ï¼Œå¹¶åˆå§‹åŒ– Agent
    """
    # åˆ›å»ºç¯å¢ƒ (æ”¯æŒæ¸²æŸ“æ¨¡å¼åˆ‡æ¢)
    # env = gym.make(cfg.env_name, hardcore=True, render_mode=render_mode)
    env = gym.make(cfg.env_name, render_mode=render_mode)
    
    # è®¾ç½®ç§å­ (ä¼ å…¥ env ä»¥åŒæ­¥ç©ºé—´ç§å­)
    all_seed(seed=cfg.seed, env=env)
    # è‡ªåŠ¨æ¢æµ‹å¹¶æ³¨å…¥ç»´åº¦ä¿¡æ¯
    n_states = env.observation_space.shape[0]
    # å…¼å®¹å¤„ç†ï¼šæœ‰äº›ç¯å¢ƒæ˜¯ Discreteï¼Œæœ‰äº›æ˜¯ Box
    if isinstance(env.action_space, gym.spaces.Discrete):
        n_actions = env.action_space.n
        setattr(cfg, "is_discrete", True)
    else:
        n_actions = env.action_space.shape[0]
        setattr(cfg, "is_discrete", False)
    setattr(cfg, "n_states", n_states)
    setattr(cfg, "n_actions", n_actions)
    setattr(cfg, "n_actors", n_actions)  # é’ˆå¯¹ä½ çš„ ActorNet è¾“å‡ºå±‚
    print(f"ğŸ¤– ç¯å¢ƒ: {cfg.env_name} | çŠ¶æ€ç»´åº¦: {n_states} | åŠ¨ä½œç»´åº¦: {n_actions}")
    # 4. åˆå§‹åŒ– Agent
    agent = Agent(cfg)
    return env, agent


def all_seed(seed=42, env=None):
    """
    è¶…è¶Šä¸‡èƒ½çš„ç§å­å‡½æ•°ï¼šç¡®ä¿ Python, Numpy, PyTorch ä»¥åŠ Gym ç¯å¢ƒå®Œå…¨åŒæ­¥
    """
    if seed <= 0:
        return
    # åŸºç¡€ Python ä¸ Numpy ç§å­
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    # PyTorch ç§å­ä¸ GPU ç¡®å®šæ€§é…ç½®
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # é’ˆå¯¹å¤š GPU
    # å½»åº•ç‰ºç‰²æ€§èƒ½æ¢å–ç¡®å®šæ€§ (SB3 ä¸¥è‹›æ¨¡å¼)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # å…³é”®ï¼šGymnasium ç¯å¢ƒç©ºé—´ç§å­
    if env is not None:
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
    print(f"âœ… å·²è®¾ç½®å…¨å±€éšæœºç§å­: {seed}")


def smooth(data, weight=0.9):
    # ç”¨äºå¹³æ»‘æ›²çº¿ï¼Œç±»ä¼¼äºTensorboardä¸­çš„smoothæ›²çº¿
    last = data[0]
    smoothed = []
    for point in data:
        smoothed_val = last * weight + (1 - weight) * point  # è®¡ç®—å¹³æ»‘å€¼
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed


def plot_rewards(rewards, cfg, tag="train"):
    sns.set_theme()
    plt.figure()  # åˆ›å»ºä¸€ä¸ªå›¾å½¢å®ä¾‹ï¼Œæ–¹ä¾¿åŒæ—¶å¤šç”»å‡ ä¸ªå›¾
    plt.title(f"{tag}ing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}")
    plt.xlabel("epsiodes")
    plt.plot(rewards, label="rewards")
    plt.plot(smooth(rewards), label="smoothed")
    plt.legend()
    plt.show()


if __name__ == "__main__":
    cfg = Config()
    env, agent = env_agent_config(cfg)
    # è®­ç»ƒ
    rewards_history = train(cfg, env, agent)
    # ç»˜å›¾
    plot_rewards(rewards_history, cfg)
    # æ¼”ç¤º
    test(cfg, agent)
